{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(directory):\n",
    "    data = {\n",
    "        'file_name': [],\n",
    "        'vgg_features': [],\n",
    "        'culture_id': [],\n",
    "        'Arousal_A_labels': [],\n",
    "        'Arousal_V_labels': [],\n",
    "        'Valence_A_labels': [],\n",
    "        'Valence_V_labels': []\n",
    "    }\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pt'):\n",
    "            # Load the tensor\n",
    "            feature_path = os.path.join(directory, filename)\n",
    "            features = torch.load(feature_path)\n",
    "\n",
    "            # Extract culture_id from the filename\n",
    "            parts = filename.split('_')\n",
    "            culture_id = next((part for part in parts if part.startswith('C')), None)\n",
    "\n",
    "            # Append data to dictionary\n",
    "            data['file_name'].append(filename[:-3])  # Remove the '.pt' extension\n",
    "            data['vgg_features'].append(features)\n",
    "            data['culture_id'].append(culture_id)\n",
    "            # Initialize placeholder values for label lists\n",
    "            data['Arousal_A_labels'].append(None)\n",
    "            data['Arousal_V_labels'].append(None)\n",
    "            data['Valence_A_labels'].append(None)\n",
    "            data['Valence_V_labels'].append(None)\n",
    "        \n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'vggpooled'\n",
    "result_df = load_features(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_adjust_features(df, base_directory):\n",
    "    hubert_features_list = []\n",
    "    adjusted_features_list = []\n",
    "    valid_indices = []  # To track indices of valid rows\n",
    "    filename_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        culture_id = row['culture_id']\n",
    "        filename = row['file_name']\n",
    "        original_features = row['vgg_features']\n",
    "\n",
    "        # Define path to the culture-specific folder\n",
    "        culture_path = os.path.join(base_directory, culture_id)\n",
    "\n",
    "        try:\n",
    "            # Locate the directory that matches the filename\n",
    "            target_folder = None\n",
    "            for folder in os.listdir(culture_path):\n",
    "                if folder == filename:\n",
    "                    target_folder = folder\n",
    "                    break\n",
    "\n",
    "            # If the matching folder is found, load the .npy file\n",
    "            if target_folder:\n",
    "                features_path = os.path.join(culture_path, target_folder, 'features.npy')\n",
    "                hubert_features = np.load(features_path)\n",
    "\n",
    "                hubert_features_list.append(hubert_features)\n",
    "                adjusted_features_list.append(original_features)\n",
    "                filename_list.append(filename)\n",
    "                valid_indices.append(idx)  # Add the index of valid row\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No matching folder for {filename} in {culture_path}\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            # If an error occurs (folder not found), we skip this row\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    # Create a new DataFrame with only the valid data\n",
    "    new_df = pd.DataFrame({\n",
    "        'vgg_features': adjusted_features_list,\n",
    "        'hubert_features': hubert_features_list,\n",
    "        'culture_id': df.loc[valid_indices, 'culture_id'].values,  # Retrieve valid culture_id values\n",
    "        'filename': filename_list\n",
    "    })\n",
    "\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust 'your_base_directory_path' to the path of your 'output_features_full_len' folder\n",
    "base_directory_path = 'output_features_full_len'\n",
    "result_df = load_and_adjust_features(result_df, base_directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_labels(df, base_directory):\n",
    "    # Create new columns in the DataFrame to store the labels\n",
    "    df['Arousal_A_labels'] = None\n",
    "    df['Arousal_V_labels'] = None\n",
    "    df['Valence_A_labels'] = None\n",
    "    df['Valence_V_labels'] = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        filename = row['filename']  # Ensure this is the correct column name\n",
    "        folder_path = os.path.join(base_directory, filename)\n",
    "\n",
    "        def load_csv_label(file_suffix):\n",
    "            try:\n",
    "                # Get the list of files that match the suffix\n",
    "                files = [f for f in os.listdir(folder_path) if f.endswith(file_suffix)]\n",
    "                if files:\n",
    "                    # Assuming there is only one such file per folder\n",
    "                    file_path = os.path.join(folder_path, files[0])\n",
    "                    label_df = pd.read_csv(file_path, usecols=[1])\n",
    "                    return label_df.iloc[:, 0].values  # Return the values of the second column\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found for {file_suffix} in {folder_path}\")\n",
    "                return None\n",
    "            return None\n",
    "\n",
    "        # Directly assign the label data to the DataFrame for the current row\n",
    "        df.at[index, 'Arousal_A_labels'] = load_csv_label('Arousal_A_Aligned.csv')\n",
    "        df.at[index, 'Arousal_V_labels'] = load_csv_label('Arousal_V_Aligned.csv')\n",
    "        df.at[index, 'Valence_A_labels'] = load_csv_label('Valence_A_Aligned.csv')\n",
    "        df.at[index, 'Valence_V_labels'] = load_csv_label('Valence_V_Aligned.csv')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust 'your_base_directory_path' to the path of your 'SEWAv02' folder\n",
    "base_directory_path = 'SEWAv02'\n",
    "result_df = load_labels(result_df, base_directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C1 = df_C1_C4 = result_df[(result_df['culture_id'] == 'C1') | (result_df['culture_id'] == 'C4')]\n",
    "\n",
    "df_C2 = result_df[result_df['culture_id']=='C2']\n",
    "df_C3 = result_df[result_df['culture_id']=='C3']\n",
    "df_C4 = result_df[result_df['culture_id']=='C4']\n",
    "df_C5 = result_df[result_df['culture_id']=='C5']\n",
    "df_C6 = result_df[result_df['culture_id']=='C6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_features(features, max_length=750):\n",
    "    # Trim the features if longer than the max_length\n",
    "    if len(features) > max_length:\n",
    "        return features[:max_length]\n",
    "    # Pad the features if shorter than the max_length\n",
    "    elif len(features) < max_length:\n",
    "        padding = np.zeros((max_length - len(features), features.shape[1]))\n",
    "        return np.vstack((features, padding))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_labels(labels):\n",
    "   # Define the desired length\n",
    "    desired_length = 750\n",
    "\n",
    "    # Initialize an empty list to store adjusted sequences\n",
    "    adjusted_labels = []\n",
    "\n",
    "    # Loop through each item in the DataFrame column\n",
    "    for label in labels:\n",
    "        if len(label) > desired_length:\n",
    "            # If the sequence is longer, crop it to the desired length\n",
    "            adjusted_label = label[:desired_length]\n",
    "        elif len(label) < desired_length:\n",
    "            # If the sequence is shorter, pad it with zeroes\n",
    "            adjusted_label = np.pad(label, (0, desired_length - len(label)), 'constant')\n",
    "        else:\n",
    "            # If it's already the desired length, use it as is\n",
    "            adjusted_label = label\n",
    "        \n",
    "        # Append the adjusted label to the list\n",
    "        adjusted_labels.append(adjusted_label)\n",
    "\n",
    "    # Convert the list of arrays to a 2D NumPy array\n",
    "    y = np.array(adjusted_labels)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(result_df):\n",
    "    audio_X = result_df['hubert_features'].apply(adjust_features)\n",
    "    X = np.array(audio_X.tolist())\n",
    "    y = adjust_labels(result_df['Arousal_A_labels'])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(df_C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc_arousal = nn.Linear(hidden_size * 2, output_size)  # Multiply by 2 for bidirectional\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 for bidirectional\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 for bidirectional\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Concatenate hidden states from both directions\n",
    "        out = torch.cat((out[:, -1, :self.hidden_size], out[:, 0, self.hidden_size:]), dim=1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Pass the concatenated hidden states through a fully connected layer for arousal prediction\n",
    "        arousal_output = self.fc_arousal(out)\n",
    "        \n",
    "        return arousal_output\n",
    "\n",
    "# Define CCCLoss function\n",
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCCLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mean_pred = torch.mean(y_pred)\n",
    "        mean_true = torch.mean(y_true)\n",
    "        cov_pred = torch.sum((y_pred - mean_pred) * (y_true - mean_true))\n",
    "        var_pred = torch.sum((y_pred - mean_pred) ** 2)\n",
    "        var_true = torch.sum((y_true - mean_true) ** 2)\n",
    "        ccc = 2 * cov_pred / (var_pred + var_true + (mean_pred - mean_true) ** 2 + 1e-8)  # Adding a small epsilon to avoid division by zero\n",
    "        return 1 - ccc\n",
    "\n",
    "def train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal):\n",
    "    input_size = X_train.shape[-1]  # Input size from the last dimension of X\n",
    "    hidden_size = 256\n",
    "    num_layers = 2\n",
    "    output_size = 750  # Number of frames\n",
    "    dropout = 0.2\n",
    "    \n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_train_arousal_tensor = torch.FloatTensor(y_train_arousal)\n",
    "    y_test_arousal_tensor = torch.FloatTensor(y_test_arousal)\n",
    "    \n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_arousal_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_arousal_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    model = BiLSTM(input_size, hidden_size, num_layers, output_size, dropout=dropout)\n",
    "    criterion = CCCLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        arousal_train_loss = 0.0\n",
    "        \n",
    "        for batch_features, batch_arousal_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            arousal_output = model(batch_features)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(arousal_output.squeeze(), batch_arousal_labels)\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            arousal_train_loss += loss.item() * batch_features.size(0)\n",
    "        \n",
    "        # Average loss over the entire dataset\n",
    "        arousal_train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Arousal Train Loss: {arousal_train_loss:.4f}\")\n",
    "    \n",
    "        # Evaluation loop\n",
    "        model.eval()\n",
    "        arousal_test_loss = 0.0\n",
    "        ccc_arousal = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_arousal_labels in test_loader:\n",
    "                arousal_output = model(batch_features)   \n",
    "                \n",
    "                # Compute CCC for arousal\n",
    "                ccc_arousal += criterion(arousal_output.squeeze(), batch_arousal_labels).item() * batch_features.size(0)\n",
    "                \n",
    "                # Accumulate loss\n",
    "                arousal_test_loss += loss.item() * batch_features.size(0)\n",
    "            \n",
    "        # Average loss over the entire test dataset\n",
    "        arousal_test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "        ccc_arousal /= len(test_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Arousal Test Loss: {arousal_test_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Arousal CCC Loss: {ccc_arousal:.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have already defined the train_bilstm function and imported necessary libraries\n",
    "\n",
    "# Replace X and y with your actual data\n",
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "bilstm_model = train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc_arousal = nn.Linear(hidden_size * 2, output_size)  # Multiply by 2 for bidirectional\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 for bidirectional\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 for bidirectional\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Concatenate hidden states from both directions\n",
    "        out = torch.cat((out[:, -1, :self.hidden_size], out[:, 0, self.hidden_size:]), dim=1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Pass the concatenated hidden states through a fully connected layer for arousal prediction\n",
    "        arousal_output = self.fc_arousal(out)\n",
    "        \n",
    "        return arousal_output\n",
    "\n",
    "# Define CCCLoss function\n",
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCCLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mean_pred = torch.mean(y_pred)\n",
    "        mean_true = torch.mean(y_true)\n",
    "        cov_pred = torch.sum((y_pred - mean_pred) * (y_true - mean_true))\n",
    "        var_pred = torch.sum((y_pred - mean_pred) ** 2)\n",
    "        var_true = torch.sum((y_true - mean_true) ** 2)\n",
    "        ccc = 2 * cov_pred / (var_pred + var_true + (mean_pred - mean_true) ** 2 + 1e-8)  # Adding a small epsilon to avoid division by zero\n",
    "        return 1 - ccc\n",
    "\n",
    "def train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal):\n",
    "    input_size = X_train.shape[-1]  # Input size from the last dimension of X\n",
    "    hidden_size = 256\n",
    "    num_layers = 2\n",
    "    output_size = 750  # Number of frames\n",
    "    dropout = 0.2\n",
    "    \n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_train_arousal_tensor = torch.FloatTensor(y_train_arousal)\n",
    "    y_test_arousal_tensor = torch.FloatTensor(y_test_arousal)\n",
    "    \n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_arousal_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_arousal_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    model = BiLSTM(input_size, hidden_size, num_layers, output_size, dropout=dropout)\n",
    "    ccc_criterion = CCCLoss()\n",
    "    rmse_criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 30\n",
    "    best_rmse = float('inf')\n",
    "    best_ccc = float('-inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        arousal_train_loss = 0.0\n",
    "        arousal_train_rmse = 0.0\n",
    "        \n",
    "        for batch_features, batch_arousal_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            arousal_output = model(batch_features)\n",
    "            \n",
    "            # Compute CCC loss\n",
    "            ccc_loss = ccc_criterion(arousal_output.squeeze(), batch_arousal_labels)\n",
    "            \n",
    "            # Compute RMSE loss\n",
    "            rmse_loss = torch.sqrt(rmse_criterion(arousal_output.squeeze(), batch_arousal_labels))\n",
    "            \n",
    "            # Compute total loss as a combination of both\n",
    "            loss = ccc_loss + rmse_loss\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            arousal_train_loss += loss.item() * batch_features.size(0)\n",
    "            arousal_train_rmse += rmse_loss.item() * batch_features.size(0)\n",
    "        \n",
    "        # Average loss over the entire dataset\n",
    "        arousal_train_loss /= len(train_loader.dataset)\n",
    "        arousal_train_rmse /= len(train_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Arousal Train Loss: {arousal_train_loss:.4f}, Arousal Train RMSE: {arousal_train_rmse:.4f}\")\n",
    "    \n",
    "        # Evaluation loop\n",
    "        model.eval()\n",
    "        arousal_test_loss = 0.0\n",
    "        arousal_test_rmse = 0.0\n",
    "        ccc_arousal = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_arousal_labels in test_loader:\n",
    "                arousal_output = model(batch_features)   \n",
    "                \n",
    "                # Compute CCC for arousal\n",
    "                ccc_arousal += ccc_criterion(arousal_output.squeeze(), batch_arousal_labels).item() * batch_features.size(0)\n",
    "                \n",
    "                # Compute RMSE for arousal\n",
    "                arousal_test_rmse += mean_squared_error(batch_arousal_labels.detach().numpy(), arousal_output.squeeze().detach().numpy(), squared=False) * batch_features.size(0)\n",
    "            \n",
    "        # Average loss over the entire test dataset\n",
    "        arousal_test_loss /= len(test_loader.dataset)\n",
    "        arousal_test_rmse /= len(test_loader.dataset)\n",
    "    \n",
    "        ccc_arousal /= len(test_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Arousal Test Loss: {arousal_test_loss:.4f}, Arousal Test RMSE: {arousal_test_rmse:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Arousal CCC Loss: {ccc_arousal:.4f}\")\n",
    "        \n",
    "        # Update best RMSE and CCC\n",
    "        if arousal_test_rmse < best_rmse:\n",
    "            best_rmse = arousal_test_rmse\n",
    "        if ccc_arousal > best_ccc:\n",
    "            best_ccc = ccc_arousal\n",
    "\n",
    "    # Report best RMSE and CCC\n",
    "    print(f\"Best Arousal RMSE: {best_rmse:.4f}\")\n",
    "    print(f\"Best Arousal CCC: {best_ccc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc_arousal = nn.Linear(hidden_size * 2, output_size)  # Multiply by 2 for bidirectional\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # Multiply by 2 for bidirectional\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # Concatenate hidden states from both directions\n",
    "        out = torch.cat((out[:, -1, :self.hidden_size], out[:, 0, self.hidden_size:]), dim=1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Pass the concatenated hidden states through a fully connected layer for arousal prediction\n",
    "        arousal_output = self.fc_arousal(out)\n",
    "        \n",
    "        return arousal_output\n",
    "\n",
    "# Define CCCLoss function\n",
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCCLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mean_pred = torch.mean(y_pred)\n",
    "        mean_true = torch.mean(y_true)\n",
    "        cov_pred = torch.sum((y_pred - mean_pred) * (y_true - mean_true))\n",
    "        var_pred = torch.sum((y_pred - mean_pred) ** 2)\n",
    "        var_true = torch.sum((y_true - mean_true) ** 2)\n",
    "        ccc = 2 * cov_pred / (var_pred + var_true + (mean_pred - mean_true) ** 2 + 1e-8)  # Adding a small epsilon to avoid division by zero\n",
    "        return 1 - ccc\n",
    "\n",
    "def train_bigru(X_train, X_test, y_train_arousal, y_test_arousal):\n",
    "    input_size = X_train.shape[-1]  # Input size from the last dimension of X\n",
    "    hidden_size = 256\n",
    "    num_layers = 2\n",
    "    output_size = 750  # Number of frames\n",
    "    dropout = 0.2\n",
    "    \n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_train_arousal_tensor = torch.FloatTensor(y_train_arousal)\n",
    "    y_test_arousal_tensor = torch.FloatTensor(y_test_arousal)\n",
    "    \n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_arousal_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_arousal_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    model = BiGRU(input_size, hidden_size, num_layers, output_size, dropout=dropout)\n",
    "    ccc_criterion = CCCLoss()\n",
    "    rmse_criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    best_rmse = float('inf')\n",
    "    best_ccc = float('-inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        arousal_train_loss = 0.0\n",
    "        arousal_train_rmse = 0.0\n",
    "        \n",
    "        for batch_features, batch_arousal_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            arousal_output = model(batch_features)\n",
    "            \n",
    "            # Compute CCC loss\n",
    "            ccc_loss = ccc_criterion(arousal_output.squeeze(), batch_arousal_labels)\n",
    "            \n",
    "            # Compute RMSE loss\n",
    "            rmse_loss = torch.sqrt(rmse_criterion(arousal_output.squeeze(), batch_arousal_labels))\n",
    "            \n",
    "            # Compute total loss as a combination of both\n",
    "            loss = ccc_loss + rmse_loss\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            arousal_train_loss += loss.item() * batch_features.size(0)\n",
    "            arousal_train_rmse += rmse_loss.item() * batch_features.size(0)\n",
    "        \n",
    "        # Average loss over the entire dataset\n",
    "        arousal_train_loss /= len(train_loader.dataset)\n",
    "        arousal_train_rmse /= len(train_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Arousal Train Loss: {arousal_train_loss:.4f}, Arousal Train RMSE: {arousal_train_rmse:.4f}\")\n",
    "    \n",
    "        # Evaluation loop\n",
    "        model.eval()\n",
    "        arousal_test_loss = 0.0\n",
    "        arousal_test_rmse = 0.0\n",
    "        ccc_arousal = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_arousal_labels in test_loader:\n",
    "                arousal_output = model(batch_features)   \n",
    "                \n",
    "                # Compute CCC for arousal\n",
    "                ccc_arousal += ccc_criterion(arousal_output.squeeze(), batch_arousal_labels).item() * batch_features.size(0)\n",
    "                \n",
    "                # Compute RMSE for arousal\n",
    "                arousal_test_rmse += mean_squared_error(batch_arousal_labels.detach().numpy(), arousal_output.squeeze().detach().numpy(), squared=False) * batch_features.size(0)\n",
    "            \n",
    "        # Average loss over the entire test dataset\n",
    "        arousal_test_loss /= len(test_loader.dataset)\n",
    "        arousal_test_rmse /= len(test_loader.dataset)\n",
    "    \n",
    "        ccc_arousal /= len(test_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Arousal Test Loss: {arousal_test_loss:.4f}, Arousal Test RMSE: {arousal_test_rmse:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Arousal CCC Loss: {ccc_arousal:.4f}\")\n",
    "        \n",
    "        # Update best RMSE and CCC\n",
    "        if arousal_test_rmse < best_rmse:\n",
    "            best_rmse = arousal_test_rmse\n",
    "        if ccc_arousal > best_ccc:\n",
    "            best_ccc = ccc_arousal\n",
    "\n",
    "    # Report best RMSE and CCC\n",
    "    print(f\"Best Arousal RMSE: {best_rmse:.4f}\")\n",
    "    print(f\"Best Arousal CCC: {best_ccc:.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arousal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi LSTM Arousal C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "model = train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(df_C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "model_gru = train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2 Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(df_C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3 Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(df_C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4 Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(df_C4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C5 Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(df_C5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C6 Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(df_C6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_valence(result_df):\n",
    "    audio_X = result_df['hubert_features'].apply(adjust_features)\n",
    "    X = np.array(audio_X.tolist())\n",
    "    y = adjust_labels(result_df['Valence_A_labels'])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data_valence(df_C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data_valence(df_C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data_valence(df_C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data_valence(df_C4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data_valence(df_C5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data_valence(df_C6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_arousal, y_test_arousal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Call the train_bilstm function with your data\n",
    "train_bilstm(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigru(X_train, X_test, y_train_arousal, y_test_arousal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(df_C6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calc_pred(model, X, y, output_file):\n",
    "\n",
    "    # Load the saved model\n",
    "    model = BiGRU(128, 256, 2, 750, dropout=0.2)\n",
    "    model.load_state_dict(torch.load('arousal_c1.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare data for prediction (X and y)\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    y_true_tensor = torch.FloatTensor(y)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_tensor)\n",
    "\n",
    "    predictions = np.array(y_pred)\n",
    "    np.savetxt(\"predictions.csv\", predictions, delimiter=\",\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_pred('arousal_c1.pt', np.array(df_C2['hubert_features'].to_list()), np.array(df_C2['Arousal_A_labels'].to_list()), 'arousal_c1_audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(result_df):\n",
    "    audio_X = result_df['hubert_features'].apply(adjust_features)\n",
    "    X = np.array(audio_X.tolist())\n",
    "    y = adjust_labels(result_df['Arousal_A_labels'])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Generating predictions based on RMSE as noise standard deviation\n",
    "audio_predictions = true_value + np.random.randn(num_instances, len(rmse_audio)) * rmse_audio\n",
    "video_predictions = true_value + np.random.randn(num_instances, len(rmse_video)) * rmse_video\n",
    "\n",
    "# Simple average fusion\n",
    "average_fusion = (audio_predictions + video_predictions) / 2\n",
    "\n",
    "# Weighted average fusion, weights based on inverse of RMSE\n",
    "weights_audio = 1 / rmse_audio\n",
    "weights_video = 1 / rmse_video\n",
    "total_weights = weights_audio + weights_video\n",
    "weighted_fusion = (audio_predictions * weights_audio + video_predictions * weights_video) / total_weights\n",
    "\n",
    "# Outputting the estimates for each culture excluding C1 (C2 to C6)\n",
    "audio_predictions, video_predictions, average_fusion, weighted_fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(model, feature_array):\n",
    "    with torch.no_grad():  # Ensuring no gradients are computed\n",
    "        # Convert numpy arrays to tensors\n",
    "        feature_tensor = torch.tensor(feature_array, dtype=torch.float32)\n",
    "        return model(feature_tensor)\n",
    "\n",
    "# Function to load features and perform predictions for a given DataFrame\n",
    "def process_and_predict(df, model):\n",
    "    # Assuming 'hubert_features' are numpy arrays and stored directly in the DataFrame\n",
    "    features = np.vstack(df['hubert_features'].values)  # Stacking arrays into a single numpy array\n",
    "    predictions = predict_with_model(model, features)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_predict(df_C2,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
